\documentclass[9pt, aspectratio=169]{beamer}

%\usetheme{uumetropolis}
\usetheme{metropolis}
\setbeamertemplate{itemize items}{\faAngleRight}

\input{../utils/preamble}

\title{Aproximación por mínimos cuadrados}
\subtitle{Motivación. Ajuste discreto lineal. Ajuste polinómico. Ajustes potencial y exponencial. Ejemplos con Python. Ajuste continuo polinómico, Funciones ortogonales. Polinomios de Legendre. Polinomios de Chebishev.}

%%%%
% Bibliografía
%%%%

\begin{document}
\maketitle

\begin{frame}{Teoría de aproximación}
    \textbf{Dos clases generales de problemas:}
    \begin{itemize}
      \item Ajuste de funciones a un conjunto de datos $\mapsto$ \textbf{aproximación discreta}.
        \item Representación de una función conocida por funciones más simples $\mapsto$ \textbf{aproximación continua}.
    \end{itemize}
\end{frame}


\begin{frame}[standout]
    \begin{center}
        {\Huge Ajuste discreto}
    \end{center}
\end{frame}

\begin{frame}
\begin{columns}[t]
\cw{0.45}
\textbf{Datos:}

\begin{center}
\begin{tabular}{cccc}
\toprule
$x_i$ & $y_i$ & $x_i$ & $y_i$ \\
\midrule
 1 & 7.07 &  6 & 18.02 \\
 2 & 6.99 &  7 & 21.69 \\
 3 & 11.37 &  8 & 23.94 \\
 4 & 14.73 &  9 & 25.07 \\
 5 & 16.03 & 10 & 28.15 \\
\bottomrule
\end{tabular}
\end{center}

\cw{0.45}
\textbf{Figura:}
\begin{center}
    \includegraphics[scale=0.40]{figs/fig-01.pdf}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\begin{columns}[t]
\cw{0.45}
\textbf{Ajuste exacto:} polinomio de grado 9

\begin{align*}
    P_{9}(x) &= 30.63 - 55.47 x + 53.23 x^2 \\
             &- 30.82 x^3 \+ 12.24 x^4 -3.21 x^5 \\
             & +0.53 x^6 - 0.053 x^7 + 0.0029 x^8 \\
             &- 6.4 \times 10^{-5} x^9
\end{align*}
\begin{center}
    \includegraphics[scale=0.40]{figs/fig-02.pdf}
\end{center} \pause

\cw{0.45}
\textbf{Aproximación lineal:}
\[ y = a_1 \, x + a_0 \]

\begin{itemize}
    \item  Problema \textbf{minimax}: 
        \[ E_{\infty}(a_0, a_1) = \max_{1 \leq i \leq 10} \{|y_i - (a_1 x_i + a_0)|\} \]  \pause
    \item Desviación absoluta: 
        \[ E_1(a_0, a_1) = \sum_{i = 1}^{10} |y_i - (a_1 x_i + a_0) | \] \pause
    \item \textbf{Mínimos cuadrados:}
    \[ E_2(a_0, a_1) = \sum_{i = 1}^{10} [y_i - (a_1 x_i + a_0)]^2 \]
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}
\begin{columns}[t]
\cw{0.55}
Para el conjunto $\{(x_i, y_i)\}_{i=1}^m$, \textbf{minimizar} respecto de \\ $a_0, a_1$:
\[ E \equiv E_2(a_0, a_1) = \sum_{i = 1}^{m} [y_i - (a_1 x_i + a_0)]^2 \] 
\pause
\[ \frac{\partial E}{\partial a_0} = 0 \txt{y} \frac{\partial E}{\partial a_1} = 0 \]
Esto es:
\[ 0 = \frac{\partial}{\partial a_0} \sum_{i = 1}^m [y_i - (a_1 x_i + a_0)]^2 = 2 \sum_{i=1}^m (y_i - a_1 x_i - a_0)(-1) \]
\[ 0 = \frac{\partial}{\partial a_1} \sum_{i = 1}^m [y_i - (a_1 x_i + a_0)]^2 = 2 \sum_{i=1}^m (y_i - a_1 x_i - a_0)(-x_i) \]
\pause

\cw{0.35}
\textbf{Ecuaciones normales:}
\begin{align*}
    a_0 \cdot m + a_1 \sum_{i=1}^m x_i &= \sum_{i=1}^m y_i \\
    a_0 \sum_{i=1}^m x_i + a_1 \sum_{i=1}^m x_i^2 &= \sum_{i=1}^m x_i \, y_i \\
\end{align*}
\pause

\textbf{Solución:}
\begin{align*}
    a_0 &= \frac{\dsum_{i=1}^m x_i^2 \dsum_{i=1}^m y_i - \dsum_{i=1}^m x_i y_i \dsum_{i=1}^m x_i}{m \left( \dsum_{i=1}^m x_i^2 \right) - \pow{\dsum_{i=1}^m x_i}{2} } \\
    a_1 &= \frac{ m \dsum_{i=1}^m x_i y_i - \dsum_{i=1}^m x_i \dsum_{i=1}^m y_i}{m \left( \dsum_{i=1}^m x_i^2 \right) - \pow{\dsum_{i=1}^m x_i}{2} } \\
\end{align*}

\end{columns}
\end{frame}

\begin{frame}
    \textbf{Ejemplo:} Encontrar la recta de mínimos cuadrados:
\begin{columns}
\cx
\begin{tabular}{ccccc}
\toprule
$x_i$ & $y_i$ & $x_i^2$ & $x_i y_i$ & $P(x_i) = 2.437 x_i + 3.903$ \\
\midrule
 1 & 7.07 &  1 & 7.07 & 6.34 \\ 
 2 & 6.99 &  4 & 13.97 & 8.78 \\ 
 3 & 11.37 &  9 & 34.10 & 11.21 \\ 
 4 & 14.73 & 16 & 58.92 & 13.65 \\ 
 5 & 16.03 & 25 & 80.14 & 16.09 \\ 
 6 & 18.02 & 36 & 108.10 & 18.52 \\ 
 7 & 21.69 & 49 & 151.85 & 20.96 \\ 
 8 & 23.94 & 64 & 191.52 & 23.40 \\ 
 9 & 25.07 & 81 & 225.62 & 25.83 \\ 
10 & 28.15 & 100 & 281.49 & 28.27 \\ 
\midrule
55 & 173.04 & 385 & 1152.77 & $E \approx 6.62$ \\
\bottomrule
\end{tabular}
\pause 

\cx
\begin{align*}
    a_0 &= \frac{385 (173.04) - 55 (1152.77)}{10 (385) - 55^2} = 3.903 \\
    a_1 &= \frac{10 (1152.77) - 55 (173.04)}{10 (385) - 55^2} = 2.437
\end{align*}

\begin{center}
    \includegraphics[scale=0.4]{figs/fig-03.pdf}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
    \begin{columns}[t]
\cx
\textbf{Mínimos cuadrados polinomiales:}
\[ P_n(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0 \]
$\{(x_i, y_i)\}, i = 1, \ldots m$, $n < m-1$. Minimizar:
\begin{align*}
    E &= \sum_{i=1}^m [y_i - P_n(x_i)]^2 \\
      &= \sum_{i=1}^m y_i^2 - 2 \sum_{i=1}^m P_n(x_i) y_i + \sum_{i=1}^m [P_n(x_i)]^2 \\
      &= \sum_{i=1}^m y_i^2 -2 \sum_{i=1}^m \left(\sum_{j=0}^n a_j x_i^j \right) y_i + \sum_{i=1}^m \pow{\sum_{j=0}^n a_j x_i^j}{2} \\
      &= \sum_{i=1}^m y_i^2 -2 \sum_{j=0}^n a_j \left( \sum_{i=1}^m y_i x_i^j \right) + \sum_{j=0}^n \sum_{k=0}^n a_j a_k \left( \sum_{i=1}^m x_i^{j+k} \right) 
\end{align*}
\pause

\cw{0.4}
Minimización: $\partial E / \partial a_j = 0, j = 0, 1, \ldots n$.
\[ 0 = \frac{\partial E}{\partial a_j} = -2 \sum_{i=1}^m y_i x_i^j + 2 \sum_{k=0}^na_k \sum_{i=1}^m x_i^{j+k} \]
$(n+1)$ \textbf{ecuaciones normales}:
\[ \sum_{k=0}^n a_k \sum_{i=1}^m x_i^{j+k} = \sum_{i=1}^m y_i x_i^j, \; j=0, 1, \ldots n \]
\end{columns}
\end{frame}

\begin{frame}
\begin{align*}
    a_0 \sum_{i=1}^m x_i^0 + a_1 \sum_{i=1}^m x_i^1 + a_2 \sum_{i=1}^m x_i^2 + \cdots + a_n \sum_{i=1}^m x_i^n &= \sum_{i=1}^m y_i x_i^0 \\ 
    a_0 \sum_{i=1}^m x_i^1 + a_1 \sum_{i=1}^m x_i^2 + a_2 \sum_{i=1}^m x_i^3 + \cdots + a_n \sum_{i=1}^m x_i^{n+1} &= \sum_{i=1}^m y_i x_i^1 \\
                                                                                                                     &\vdots \\
    a_0 \sum_{i=1}^m x_i^n + a_1 \sum_{i=1}^m x_i^{n+1} + a_2 \sum_{i=1}^m x_i^{n+2} + \cdots + a_n \sum_{i=1}^m x_i^{2n} &= \sum_{i=1}^m y_i x_i^n  
\end{align*}

\centering \alert{Solución única:} $x_i \neq x_j \quad \forall i \neq j$.
\end{frame}

\begin{frame}
\begin{columns}
\cx
    \textbf{Ejemplo:}
\begin{center}
\begin{tabular}{ccc}
    \toprule
    $i$ & $x_i$ & $y_i$ \\
    \midrule
    1 & 0 & 1.0000 \\
    2 & 0.25 & 1.2840 \\
    3 & 0.50 & 1.6487 \\
    4 & 0.75 & 2.1170 \\
    5 & 1.00 & 2.7183 \\
    \bottomrule
\end{tabular} 
\end{center} \pause

$n = 2, m = 5$
\begin{align*}
    5 a_0 + 2.5 a_1 + 1.875 a_2 &= 8.7680 \\
    2.5 a_0 + 1.875 a_1 + 1.5625 a_2 &= 5.4514 \\
    1.875 a_0 + 1.5625 a_1 + 1.3828 a_2 &= 4.4015
\end{align*} \pause

\textbf{Solución:}
\[a_0 = 1.005, \; a_1 = 0.8642, \; a_2 = 0.8437 \] 

\cx
\[P_2(x)= 1.005 + 0.8642 x + 0.8437 x^2 \]

\textbf{Error total:}
\[ E = \sum_{i=1}^5[y_i - P_2(x_i)]^2 = 2.74 \mul 10^{-4} \]
\pause

\begin{center}
    \includegraphics[scale=0.4]{figs/fig-04.pdf}
\end{center}

\end{columns}
\end{frame}


\begin{frame}
\begin{columns}[t]
\cx 
\textbf{Relación exponencial:}
\[ y = b \, e^{a x} \]
Minimizar:
\[ E = \sum_{i=1}^m (y_i - b e^{a x_i})^2 \]
Ecuaciones normales:
\begin{align*}
0 &= \frac{\partial E}{\partial b} = 2 \sum_{i=1}^m (y_i - b e^{a x_i})(-e^{a x_i}) \\
0 &= \frac{\partial E}{\partial a} = 2 \sum_{i=1}^m (y_i - b e^{a x_i})(-b x_i e^{a x_i}) \\
\end{align*}
\alert{Alternativa:}
\[ \ln y = \ln b + a x \]

\cx
\textbf{Relación potencial:}
\[ y = b \, x^{a} \]
Minimizar:
\[ E = \sum_{i=1}^m (y_i - b x_i^a)^2 \]
Ecuaciones normales:
\begin{align*}
0 &= \frac{\partial E}{\partial b} = 2 \sum_{i=1}^m (y_i - b x_i^{a})(-x_i^{a}) \\
0 &= \frac{\partial E}{\partial a} = 2 \sum_{i=1}^m (y_i - b x_i^{a})[-b \ln(x_i) x_i^{a}] \\
\end{align*}
\alert{Alternativa:}
\[ \ln y = \ln b + a \ln x \]
\end{columns}
\end{frame}


\begin{frame}
\begin{columns}
\cx
\textbf{Ejemplo:}

\begin{center}
\begin{tabular}{cccccc}
\toprule
$i$ & $x_i$ & $y_i$ & $\ln y_i$ & $x_i^2$ & $x_i \ln y_i$ \\
\midrule
 1 & 1.00 & 5.10 & 1.63 & 1.00 & 1.63 \\ 
 2 & 1.25 & 5.79 & 1.76 & 1.56 & 2.20 \\
 3 & 1.50 & 6.53 & 1.88 & 2.25 & 2.81 \\
 4 & 1.75 & 7.45 & 2.01 & 3.06 & 3.51 \\ 
 5 & 2.00 & 8.46 & 2.14 & 4.00 & 4.27 \\
\midrule
   & 7.50 & & 9.41 & 11.88 & 14.42 \\
\bottomrule
\end{tabular}
\end{center}

\cx
\begin{center}
    \begin{overprint}
    \onslide<1-2> \includegraphics[scale=0.4]{figs/fig-05.pdf}
    \onslide<3> \includegraphics[scale=0.4]{figs/fig-05b.pdf} 
\end{overprint}
\end{center}
\end{columns} \pause

\begin{columns}
\cx
    \[ y = b e^{ax} \Rightarrow \ln y = \ln b + a x \]

\begin{align*}
    a &= \frac{5 (14.42) - (7.5)(9.41)}{5 (11.88) - (7.5)^2} = 0.5057 \\
    \ln b&= \frac{(11.88)(9.41) - (14.42)(7.5)}{5 (11.88) - (7.5)^2} = 1.122 
\end{align*}

\cx
$\ln b = 1.122 \rightarrow b = e^{1.122} = 3.071$
\[ y = 3.071 e^{0.5056 x} \]
\end{columns}
\end{frame}

\begin{frame}[fragile]
\begin{columns}
\column{0.45\textwidth}

\textbf{Python: ejemplo 1}
\pycode[firstline=5, lastline=14]{code/ejemplo-01.py}

\begin{shell}
$ ./ejemplo-01.py 
 
2.437 x + 3.903
a_0 = 3.90314, a_1 = 2.43661
\end{shell}

\cx
\begin{center}
\includegraphics[scale=0.40]{code/ejem-01.pdf}
\end{center}

\end{columns}
\end{frame}

\begin{frame}[fragile]
\begin{columns}
\column{0.45\textwidth}

\textbf{Python: ejemplo 2}
\pycode[firstline=5, lastline=15]{code/ejemplo-02.py}

\begin{shell}
$ ./ejemplo-02.py 

0.5057 x + 1.122
ln(b) = 1.12249, a = 0.50572
\end{shell}

\cx
\begin{center}
\includegraphics[scale=0.40]{code/ejem-02.pdf}
\end{center}

\end{columns}
\end{frame}

\begin{frame}[fragile]
\begin{columns}
\column{0.45\textwidth}

\textbf{Python: ejemplo 3}
\pycode[firstline=5, lastline=16]{code/ejemplo-03.py}

\begin{shell}
$ ./ejemplo-03.py 
      2
2.55 x - 3.099 x - 1.205
a_0 = -1.20479, a_1 = -3.09868, a_2 = 2.54970
\end{shell}

\cx
\begin{center}
\includegraphics[scale=0.40]{code/ejem-03.pdf}
\end{center}

\end{columns}
\end{frame}

\begin{frame}[fragile]
\begin{columns}
\column{0.45\textwidth}

\textbf{Python: ejemplo 4}
\pycode[firstline=5, lastline=18]{code/ejemplo-04.py}

\begin{shell}
> ./ejemplo-04.py 
[2.50685815 1.21831291 0.51137751]
\end{shell}

\cx
\begin{center}
\includegraphics[scale=0.40]{code/ejem-04.pdf}
\end{center}
\end{columns}
\end{frame}

\begin{frame}[standout]\
    \begin{center}
        {\Huge Ajuste continuo}
    \end{center}
\end{frame}

\begin{frame}
\begin{columns}[t]
\cw{0.45}

$f(x) \in \C[a, b]$, hallar $P_n(x)$ que minimize:
\[ \int_a^b [f(x) - P_n(x)]^2 \, dx \]
\begin{center}
    \includegraphics[width=1.0\textwidth]{figs/fig-06.pdf}
\end{center}
\pause

\[P_n(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0 = \sum_{k=0}^n a_k x^k \]
\[ E \equiv E_2(a_0, a_1, \cdots, a_n) = \int_a^b \pow{f(x) - \sum_{k=0}^n a_k x^k}{2} \, dx \]  \pause

\cw{0.45}
\[ \frac{\partial E}{\partial a_j} = 0 \txt{para cada} j = 0, 1, \cdots, n \]

\begin{multline*}
    E = \int_a^b [f(x)]^2 dx -2 \sum_{k=0}^n a_k \int_a^b x^k f(x) \, dx \\
    + \int_a^b \pow{\sum_{k=0}^n a_k x^k}{2} \, dx
\end{multline*}

\[ \frac{\partial E}{\partial a_j} = -2 \int_a^b x^j f(x) \, dx +2 \sum_{k=0}^n a_k \int_a^b x^{j+k} \, dx \]
\alert{Ecuaciones normales} lineales ($n+1$):
\[ \sum_{k=0}^n a_k \int_a^b x^{j+k} \, dx = \int_a^b x^j f(x) \, dx \]
\flushright para cada $j = 0, 1, \cdots , n$.
\end{columns}
\end{frame}

\begin{frame}
    \textbf{Ejemplo:} aproximar $f(x) = \sen \pi x$ por un polinomio de grado 2 en $[0, 1]$.

\begin{columns}
\cw{0.6}
Ecuaciones normales:
\begin{align*}
    a_0 \int_0^1 1 \, dx + a_1 \int_0^1 x \, dx + a_2 \int_0^1 x^2 \, dx &= \int_0^1 \sen \pi x \, dx \\
    a_0 \int_0^1 x \, dx + a_1 \int_0^1 x^2 \, dx + a_2 \int_0^1 x^3 \, dx &= \int_0^1 x \, \sen \pi x \, dx \\
    a_0 \int_0^1 x^2 \, dx + a_1 \int_0^1 x^3 \, dx + a_2 \int_0^1 x^4 \, dx &= \int_0^1 x^2 \, \sen \pi x \, dx \\
\end{align*}

\begin{mathcols}
a_0 + \frac{1}{2} a_1 + \frac{1}{3} a_2 &= \frac{2}{\pi} \\
\frac{1}{2} a_0 + \frac{1}{3} a_1 + \frac{1}{4} a_2 &= \frac{1}{\pi} \\
\frac{1}{3}a_0 + \frac{1}{4} a_1 + \frac{1}{5} a_2 &= \frac{\pi^2 - 4}{\pi^3} \\
\changecol
&\txt{Solución} & \\
    a_0 &= \frac{12 \pi^2 - 120}{\pi^3} \approx -0.050465 \\
    a_1 &= -a2 = \frac{720 - 60 \pi^2}{\pi^3} \approx 4.12251
\end{mathcols}

\cw{0.4}
\begin{center}
    \includegraphics[width=1.0\textwidth]{figs/fig-07.pdf}
\end{center}

\end{columns}
\end{frame}

\begin{frame}
    \begin{columns}[t]
\cw{0.4}
\textbf{Problemas:}
\begin{itemize}
\item matriz de Hilbert
\begin{align*}
    H_{ij} &= \int_a^b x^{j+k} \, dx = \frac{b^{j+k+1} - a^{j+k+1}}{j + k + 1} \\
    \bm{H} &= \begin{bmatrix}
        1 & \frac{1}{2} & \frac{1}{3} \\
        \frac{1}{2}& \frac{1}{3} & \frac{1}{4} \\
        \frac{1}{3}& \frac{1}{4} & \frac{1}{5} \\
    \end{bmatrix} \\
        \cond(\bm{H}) &\approx 524.05678
\end{align*} \pause
\item No es fácil obtener $P_{n+1}(x)$ si ya tenemos $P_n(x)$
\end{itemize} \pause

\cw{0.55}
\begin{definition}[ Funciones linealmente independientes ]
Se dice que el conjunto de funciones $\{ \phi_0, \cdots, \phi_n \}$ es \textbf{linealmente independiente} (LI) en $[a, b]$ si
\[ P(x) = c_0 \phi_0(x) + c_1 \phi_1(x) + \cdots + c_n \phi_n(x) = 0, \forall x \in [a, b] \]
entonces $c_0 = c_1 = \cdots = c_n = 0$. De lo contrario, se dice que el conjunto de funciones es \textbf{linealmente dependiente}.
\end{definition}  \pause

\begin{theorem}[Polinomios LI]
    Si para cada $j = 0, 1, \cdots, n$, $\phi_j(x)$ es un polinomio de grado $j$, entonces el conjunto $\{ \phi_0, \cdots, \phi_n \}$ es LI en cualquier intervalo $[a, b]$.
\end{theorem}

\end{columns}
\end{frame}

\begin{frame}
    \textbf{Ejemplo.} Si $\phi_0(x) = 2, \phi_1(x) = x-3, \phi_2(x) = x^2 + 2x+7$ y $Q(x) = a_0 + a_1 x + a_2 x^2$, mostrar que existen constantes $c_0, c_1, c_2$ tales que $Q(x) = c_0 \phi_0(x) + c_1 \phi_1(x) + c_2 \phi_2(x)$.
\vspace{1em} \pause

\begin{columns}[t]
\cx
Por el teorema anterior, $\{\phi_0, \phi_1, \phi_2 \}$ es LI en cualquier $[a, b]$. Además:
\begin{align*}
1 &= \frac{1}{2}\phi_0(x) \\
x &= \phi_1(x) + 3 = \phi_1(x) + \frac{3}{2} \phi_0(x) \\
    x^2 &= \phi_2(x) - 2 x - 7 \\ 
        &= \phi_2(x) - 2 \left[ \phi_1(x) + \frac{3}{2} \phi_0(x) \right] - 7 \left[ \frac{1}{2} \phi_0(x) \right] \\
        &= \phi_2(x) -2 \phi_1(x) - \frac{13}{2} \phi_0(x)
\end{align*} \pause

\cx
Entonces:
\begin{align*}
    Q(x) &= a_0 \left[ \frac{1}{2} \phi_0 \right] + a_1 \left[ \phi_1(x) + \frac{3}{2} \phi_0(x)  \right] \\
         &+ a_2 \left[ \phi_2(x) - 2\phi_1(x) - \frac{13}{2} \phi_0(x) \right] \\
         &= \left( \frac{1}{2} a_0 + \frac{3}{2} a_1 - \frac{13}{2} a_2 \right) \phi_0(x)\\
         &+ [a_1 - 2 a_2] \phi_1(x) + a_2 \phi_2(x)
\end{align*}
\end{columns} \pause

\begin{theorem}[]
    Si $\Pi_n$ denota el conjunto de todos los polinomios de grado a lo sumo $n$, y $\{\phi_0(x), \phi_1(x), \cdots, \phi_n(x) \}$ es un conjunto de polinomios LI en $\Pi_n$, entonces \textbf{cualquier} polinomio en $\Pi_n$ se puede escribir como combinación lineal de $\phi_0(x), \phi_1(x), \cdots, \phi_n(x)$.
\end{theorem}

\end{frame}

\begin{frame}
    \begin{columns}[t]
\cw{0.45}
\begin{definition}[Función de peso]
    Una función integrable $w$ se denomina \textbf{función de peso} en el intervalo $I$ si $w(x) \geq 0, \forall x \in I$, pero $w(x) \not\equiv 0$ en cualquier subintervalo de $I$
\end{definition} \pause

Ejemplo: 
\[ w(x) = \frac{1}{\sqrt{1 - x^2}} \]
\begin{center}
    \includegraphics[width=0.9\textwidth]{figs/fig-08.pdf}
\end{center} \pause

\cw{0.55}
\begin{definition}[Funciones ortogonales]
    Se dice que $\{\phi_0, \phi_1, \cdots, \phi_n\}$ es un \textbf{conjunto ortogonal de funciones} en el intervalo $[a, b] $ respecto de la función de peso $w(x)$ si
    \[ \langle \phi_k, \phi_j \rangle_w = \int_a^b w(x) \phi_k(x) \phi_j(x) \, dx = 
        \begin{cases}
            0, & j \neq k, \\
            \alpha_j > 0, & j = k
        \end{cases}
    \]
Si además $\alpha_j = 1$ para cada $j = 0, 1, 2, \cdots, n$, se dice que el conjunto es \textbf{ortonormal}.
\end{definition} \pause

Ejemplo: $\{ \cos nx, \sen mx \}, n, m = 0, 1,  \cdots$ es ortogonal en $[-\pi, \pi]$ con $w(x) = 1$:
\begin{mathcols}
    \langle \cos nx, \cos mx \rangle_w &= 0 \\
    \langle \sen nx, \sen mx \rangle_w &= 0 \\
    \langle \cos nx, \sen mx \rangle_w &= 0 \\
    \changecol
    \langle \cos nx, \cos nx \rangle_w &= \pi \\
    \langle \sen nx, \sen nx \rangle_w &= \pi \\
    n \neq m
\end{mathcols}
\end{columns}
\end{frame}

\begin{frame}
\begin{columns}[t]
\cx
\begin{theorem}[]
    Si $\{\phi_0, \cdots, \phi_n \}$ es un conjunto ortogonal de funciones en un intervalo $[a, b]$ respecto de la función de peso $w(x)$, entonces la aproximación por mínimos cuadrados para $f$ en $[a, b]$ respecto de $w$ es:
    \[P(x) = \sum_{j=0}^n a_j \phi_j(x) \]
    donde para cada $j = 0, 1, \cdots, n$:
    \[ a_j = \frac{1}{\alpha_j} \langle f, \phi_j \rangle_w \]
\end{theorem} \pause

\cx
\begin{theorem}[]
    El conjunto de polinomios $\{\phi_0, \phi_1, \cdots, \phi_n\}$ definido de la siguiente forma es ortogonal en $[a, b]$ respecto de la función de peso $w(x)$:
    \[ \phi_0(x) \equiv 1, \; \phi_1(x) = x - B_1, \forall x \in [a, b] \]
    donde 
    \[ B_1 = \frac{ \langle x \phi_0, \phi_0 \rangle_w} {\langle \phi_0, \phi_0 \rangle_w} \]
    y cuando $k \geq 2$:
    \[ \phi_k(x) = (x - B_k) \phi_{k-1}(x) - C_k \phi_{k-2}(x), \, \forall x \in [a, b] \]
    donde
    \begin{mathcols}
        B_k &= \frac{ \langle x \phi_{k-1}, \phi_{k-1} \rangle_w } { \langle \phi_{k-1}, \phi_{k-1} \rangle_w } 
        \changecol
        C_k = \frac{ \langle x \phi_{k-1}, \phi_{k-2} \rangle_w }{\langle \phi_{k-2}, \phi_{k-2} \rangle_w }
    \end{mathcols} 
\end{theorem}
\centering {\small (Proceso de Gram-Schmidt)}
\end{columns}
\end{frame}

\begin{frame}
\begin{columns}[t]
\cx
\textbf{Polinomios de Legendre.}

$\{P_n(x) \}$ es ortogonal en $[-1, 1]$ con $w(x) \equiv 1$. Usando Gram-Schmidt con $P_0(x) \equiv 1$:
\[ B_1 = \frac{\int_{-1}^1 x \, dx}{\int_{-1}^1 dx} = 0, \, P_1(x) = (x - B_1) P_0(x) = x \] \pause
Luego:
\[ B_2 = \frac{\int_{-1}^1 x^3 \, dx}{\int_{-1}^1 x^2 \,dx} = 0, \, C_2(x) = \frac{\int_{-1}^1 x^2 \, dx}{\int_{-1}^1 1 \,dx} = \frac{1}{3} \]
\begin{align*}
    P_2(x) &= (x - B_2) P_1(x) - C_2 P_0(x) \\
           &= (x - 0) x - \frac{1}{3} 1 \\
           &= x^2 - \frac{1}{3}
\end{align*} \pause

\cx
\begin{align*}
    P_3(x) &= x P_2(x) - \frac{4}{15} P_1(x) = x^3 - \frac{3}{5} x \\
    P_4(x) &= x^4 - \frac{6}{7} x^2 + \frac{3}{35}, \, P_5(x) = x^5 - \frac{10}{9} x^3 + \frac{5}{21} x
\end{align*} \pause

\begin{center}
    \includegraphics[width=0.9\textwidth]{figs/fig-09.pdf}
\end{center}
\end{columns}
\end{frame}

\begin{frame}[fragile]
\begin{columns}[t]
\cw{0.45}
\textbf{Ejemplo:} Python
\pycode[firstline=1, lastline=22]{figs/plot-05.py}

\cw{0.5}
\pycode[firstline=24]{figs/plot-05.py}
\end{columns}
\end{frame}

\begin{frame}
    \begin{center}
        \includegraphics<1>[width=0.8\textwidth]{figs/fig-10-1.pdf}
        \includegraphics<2>[width=0.8\textwidth]{figs/fig-10-2.pdf}
        \includegraphics<3>[width=0.8\textwidth]{figs/fig-10-3.pdf}
        \includegraphics<4>[width=0.8\textwidth]{figs/fig-10-4.pdf}
    \end{center}
\end{frame}

\begin{frame}
\begin{columns}[t]
\cx
\textbf{Polinomios de Chebyshev.}

$\{ T_n(x)\}$ es ortogonal en $(-1, 1)$ con función de peso $w(x) = (1-x^2)^{-1/2}$. Para $x \in [-1, 1]$:
\[ T_n(x) = \cos[n \arccos x], \; n \geq 0 \] \pause \vspace{-2em}
\[ T_0(x) = \cos 0 = 1 \txt{y} T_1(x) =\cos(\arccos x) = x \] \pause
Para $n \geq 1$, $\theta = \arccos x$:
\[ T_n(\theta(x)) \equiv T_n(\theta) = \cos (n \theta), \: \theta \in [0, \pi] \]
Relación de recurrencia:
\begin{align*}
T_{n+1}(\theta) &= \cos(n+1) \theta = \cos \theta \cos(n \theta) - \sen \theta \sen (n \theta) \\
T_{n-1}(\theta) &= \cos(n-1) \theta =  \cos \theta \cos(n \theta) + \sen \theta \sen (n \theta) 
\end{align*}
Sumando: 
\[ T_{n+1}(\theta) = 2 \cos \theta \cos(n \theta) - T_{n-1}(\theta) \] \pause

\cx
Regresando a $x = \cos \theta$, para $n \geq 1$:
\begin{align*}
    T_{n+1} &= 2 x \cos(n \arccos x) - T_{n-1}(x) \\
    T_{n+1}(x) &= 2 x T_n(x) - T_{n-1}(x)
\end{align*} \pause
Dado que $T_0(x) = 1$ y $T_1(x) = x$:
\begin{align*}
T_2(x) &= 2 x T_1(x) - T_0(x) = 2 x^2 -1 \\
T_3(x) &= 2 x T_2(x) - T_1(x) = 4 x^3 - 3 x \\
T_4(x) &= 2 x T_3(x) - T_2(x) = 8 x^4 - 8 x^2 + 1
\end{align*}

\begin{center}
    \includegraphics[width=0.8\textwidth]{figs/fig-11.pdf}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\begin{columns}[t]
\cx
\textbf{ Ortogonalidad de polinomios de Chebyshev }

\[ \int_{-1}^1 \frac{T_n(x) T_m(x)}{\sqrt{1 - x²}} \, dx = \int_{-1}^1 \frac{\cos(n \arccos x) \cos(m \arccos x)}{\sqrt{1 - x^2}} dx \]
Reintroducimos $\theta = \arccos x$:
\begin{align*}
d\theta &= -\frac{1}{\sqrt{1 - x^2}} dx \\
\int_{-1}^1 \frac{T_n(x) T_m(x)}{\sqrt {1 - x^2}} dx &= -\int_{\pi}^0 \cos(n \theta) \cos(m \theta) \, d\theta \\
  &= \int_{0}^{\pi} \cos(n \theta) \cos(m \theta) \, d\theta 
\end{align*}
Para $n \neq m$:
\[ \cos(n \theta) \cos(m \theta) = \frac{1}{2}[\cos(n+m) \theta + \cos(n-m)\theta] \]

\cw{0.25}
\vspace{7em}

Entonces:
\begin{align*}
    \langle T_n, T_m \rangle_w &= \frac{1}{2} \int_0^{\pi} \cos[(n+m) \theta] \, d\theta \\
      &+\frac{1}{2} \int_0^{\pi} \cos[(n-m) \theta] \, d\theta \\
      &= \left[ \frac{\sen[(n+m) \theta]}{2(n+m)} + \frac{\sen[(n-m) \theta]}{2(n-m)} \right]_0^{\pi}  \\
      &= 0
\end{align*}
y
\[ \langle T_n, T_n \rangle_w = 
    \begin{cases}
    \frac{\pi}{2}, &n \geq 1 \\
    \pi, &n = 0
\end{cases} \]
\end{columns}
\end{frame}

\begin{frame}
\begin{columns}[c]
\cx
\textbf{Reducción de grado de polinomio:}

\[ q(x) = x^5 - 4 x^4 + x^3 - x - 3 \]

Aproximación por:
\[ P_4(x) = c_0 + c_1 T_1(x) + c_2 T_2(x) + c_3 T_3(x) + c_4 T_4(x) \]
donde
\[ c_j = \frac{\langle q, T_j \rangle_w}{\langle T_j, T_j \rangle_w} \] \pause

\textbf{Resultado:} ver \texttt{code/plot-12.py}.
\[ P(x) = - 4.0 x^4 + 2.25 x^3 - 1.31 x - 3.0 \]

\cx
\begin{center}
    \includegraphics[height=1.0\textheight]{figs/fig-12.pdf}
\end{center}

\end{columns}
\end{frame}


\section*{Bibliografía}
\begin{frame}{Lecturas recomendadas}
\begin{itemize}
    \item \fullcite{burden2017}. Capítulo 8.
    \item \fullcite{salgado2023}. Capítulo 11.
    \item \fullcite{quarteroni2000}. Capítulo 10.
    \item \fullcite{kreyszig2011}. Capítulo 25.9.
\end{itemize}
\end{frame}
\end{document}

